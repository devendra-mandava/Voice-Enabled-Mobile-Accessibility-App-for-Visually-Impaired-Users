{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import glob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLMIN49DeqO5",
        "outputId": "3177c6bd-a8e4-44c4-8c95-49128f85b1a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<_io.TextIOWrapper name='/content/drive/MyDrive/Colab Notebooks/original/questions/v2_OpenEnded_mscoco_val2014_questions.json' mode='r' encoding='UTF-8'>\n",
            "<_io.TextIOWrapper name='/content/drive/MyDrive/Colab Notebooks/original/questions/v2_OpenEnded_mscoco_train2014_questions.json' mode='r' encoding='UTF-8'>\n",
            "<_io.TextIOWrapper name='/content/drive/MyDrive/Colab Notebooks/original/questions/v2_OpenEnded_mscoco_test2015_questions.json' mode='r' encoding='UTF-8'>\n",
            "<_io.TextIOWrapper name='/content/drive/MyDrive/Colab Notebooks/original/questions/v2_OpenEnded_mscoco_test-dev2015_questions.json' mode='r' encoding='UTF-8'>\n",
            "Total words: 331640\n"
          ]
        }
      ],
      "source": [
        "orig_path = \"/content/drive/MyDrive/DL_Project/original\"\n",
        "dest_path = \"/content/preprocessed\"\n",
        "\n",
        "# Maximum number of top answers to consider\n",
        "max_answers = 1000\n",
        "\n",
        "def generate_query_vocabulary():\n",
        "\n",
        "    dataset_files = os.listdir(orig_path + '/questions')\n",
        "    regex_pattern = re.compile(r'\\\\W+')\n",
        "\n",
        "    query_words = []\n",
        "\n",
        "    for file_name in dataset_files:\n",
        "        file_path = os.path.join(orig_path, 'questions', file_name)\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'r') as file_obj:\n",
        "                print(file_obj)\n",
        "                question_data = json.load(file_obj)\n",
        "                questions = question_data['questions']\n",
        "\n",
        "        except (IOError, SyntaxError):\n",
        "            break\n",
        "\n",
        "        for idx, query in enumerate(questions):\n",
        "            split_question = regex_pattern.split(query['question'].lower())\n",
        "            cleaned_words = [word.strip() for word in split_question if len(word.strip()) > 0]\n",
        "\n",
        "            query_words.extend(cleaned_words)\n",
        "\n",
        "    # Remove duplicates and sort the vocabulary\n",
        "    query_words = list(set(query_words))\n",
        "    query_words.sort()\n",
        "\n",
        "    # Add empty strings at the start of the vocabulary\n",
        "    query_words.insert(0, '')\n",
        "    query_words.insert(1, '')\n",
        "\n",
        "    if not os.path.exists(dest_path):\n",
        "        os.makedirs(dest_path)\n",
        "\n",
        "    # Write the vocabulary to a file\n",
        "    vocab_file_path = os.path.join(dest_path, 'Questions', 'question_vocabs.txt')\n",
        "    with open(vocab_file_path, 'w') as file_obj:\n",
        "        file_obj.writelines([word + '\\\\n' for word in query_words])\n",
        "\n",
        "    print(f\"Total words: {len(query_words)}\")\n",
        "\n",
        "generate_query_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uVhqjSF932w",
        "outputId": "54b19856-e620-48bf-c77e-dd6bc0b0a2e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvOrOmpZhgdA",
        "outputId": "e5465047-5a23-4749-d8e4-b41d0f229a4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of total words of answers: 26480\n",
            "Keep top 1000 answers into vocabulary\n"
          ]
        }
      ],
      "source": [
        "def create_answer_vocabulary(max_answers_count):\n",
        "\n",
        "    answer_counts = defaultdict(int)\n",
        "    annotation_files = os.listdir(orig_path + '/annotations')\n",
        "\n",
        "    for file_name in annotation_files:\n",
        "        file_path = os.path.join(orig_path, 'annotations', file_name)\n",
        "\n",
        "        try:\n",
        "            with open(file_path, 'r') as file_obj:\n",
        "                data = json.load(file_obj)\n",
        "        except (IOError, SyntaxError):\n",
        "            break\n",
        "\n",
        "        annotations = data['annotations']\n",
        "\n",
        "        for entry in annotations:\n",
        "            answer = entry['multiple_choice_answer']\n",
        "            if re.search(r'[^\\w\\s]', answer):\n",
        "                continue\n",
        "\n",
        "            answer_counts[answer] += 1\n",
        "\n",
        "    sorted_answers = sorted(answer_counts, key=answer_counts.get, reverse=True)\n",
        "    top_answers = [''] + sorted_answers[:max_answers_count - 1]\n",
        "\n",
        "    annotation_vocab_dir = os.path.join(dest_path, 'Annotations')\n",
        "    if not os.path.exists(annotation_vocab_dir):\n",
        "        os.makedirs(annotation_vocab_dir)\n",
        "\n",
        "    vocab_file_path = os.path.join(annotation_vocab_dir, 'annotation_vocabs.txt')\n",
        "    with open(vocab_file_path, 'w') as file_obj:\n",
        "        file_obj.writelines([ans + '\\n' for ans in top_answers])\n",
        "\n",
        "    print(f'The num of total words of answers: {len(sorted_answers)}')\n",
        "    print(f'Keep top {max_answers_count}')\n",
        "\n",
        "create_answer_vocabulary(1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7NhitsJhpK8",
        "outputId": "4f16a043-bb12-4452-b423-e9f55da54570"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<_io.TextIOWrapper name='/content/drive/MyDrive/Colab Notebooks/original/questions/v2_OpenEnded_mscoco_val2014_questions.json' mode='r' encoding='UTF-8'>\n",
            "<_io.TextIOWrapper name='/content/drive/MyDrive/Colab Notebooks/original/questions/v2_OpenEnded_mscoco_train2014_questions.json' mode='r' encoding='UTF-8'>\n",
            "<_io.TextIOWrapper name='/content/drive/MyDrive/Colab Notebooks/original/questions/v2_OpenEnded_mscoco_test2015_questions.json' mode='r' encoding='UTF-8'>\n",
            "<_io.TextIOWrapper name='/content/drive/MyDrive/Colab Notebooks/original/questions/v2_OpenEnded_mscoco_test-dev2015_questions.json' mode='r' encoding='UTF-8'>\n",
            "Total words: 331640\n",
            "The number of total words of answers: 26480\n",
            "Keep top 1000 answers into vocabulary\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  \n",
        "    generate_query_vocabulary()  # Generate the vocabulary for questions\n",
        "    create_answer_vocabulary(max_answers_count=max_answers)  # Generate the vocabulary for answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "U2BUYKqhhyle"
      },
      "outputs": [],
      "source": [
        "output_path = \"/user/dmandava/DL_Project1/preprocessed\"\n",
        "\n",
        "# Original questions directory path\n",
        "questions_path = \"/content/drive/MyDrive/DL_Project/original/questions\"\n",
        "\n",
        "# Original annotations directory path\n",
        "annotations_path = \"/content/drive/MyDrive/DL_Project/original/annotations\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OxfMZFU8h5-L"
      },
      "outputs": [],
      "source": [
        "def find_top_answer(annotation_answer):\n",
        "    annotation_vocab_path = os.path.join(output_path, 'Annotations', 'annotation_vocabs.txt')\n",
        "\n",
        "    with open(annotation_vocab_path, 'r') as file_obj:\n",
        "        top_answers = [line.strip() for line in file_obj]\n",
        "\n",
        "    if annotation_answer not in top_answers:\n",
        "        annotation_answer = ''\n",
        "        find_top_answer.unknown_answers_count += 1\n",
        "\n",
        "    return annotation_answer\n",
        "\n",
        "# Initialize the unknown answers count\n",
        "find_top_answer.unknown_answers_count = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cCAT1WldiFFk"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(question_file, annotations_dir, is_labeled):\n",
        "\n",
        "    with open(question_file, 'r') as file_obj:\n",
        "        data = json.load(file_obj)\n",
        "\n",
        "    questions = data['questions']\n",
        "\n",
        "    if data['data_subtype'] == 'test-dev2015':\n",
        "        file_prefix = 'test2015'\n",
        "    else:\n",
        "        file_prefix = data['data_subtype']\n",
        "\n",
        "    if is_labeled:\n",
        "        # For labeled data (training or validation set)\n",
        "        annotation_pattern = os.path.join(annotations_dir, f'*{file_prefix}*.json')\n",
        "        annotation_path = glob.glob(annotation_pattern)[0]\n",
        "\n",
        "        with open(annotation_path, 'r') as file_obj:\n",
        "            annotations = json.load(file_obj)['annotations']\n",
        "\n",
        "        question_annotations = {ann['question_id']: ann for ann in annotations}\n",
        "\n",
        "        find_top_answer.unknown_answers_count = 0\n",
        "\n",
        "    dataset = [None] * len(questions)\n",
        "\n",
        "    for idx, question in enumerate(questions):\n",
        "        if (idx + 1) % 10000 == 0:\n",
        "            print(f'Processing {data[\"data_subtype\"]} data: {idx + 1}/{len(questions)}')\n",
        "\n",
        "        question_id = question['question_id']\n",
        "        question_sentence = question['question']\n",
        "        image_id = question['image_id']\n",
        "        image_name = f'COCO_{file_prefix}_{image_id:012d}.jpg'\n",
        "\n",
        "        data_entry = [image_name, question_sentence]\n",
        "\n",
        "        if is_labeled:\n",
        "            annotation_answer = question_annotations[question_id]['multiple_choice_answer']\n",
        "            answer = find_top_answer(annotation_answer)\n",
        "            data_entry.append(answer)\n",
        "\n",
        "        dataset[idx] = data_entry\n",
        "\n",
        "    if is_labeled:\n",
        "        print(f'Total {find_top_answer.unknown_answers_count} out of {len(questions)} answers are unknown')\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "V4FCAqK-ilxY"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(question_file, annotations_dir, is_labeled):\n",
        "\n",
        "    with open(question_file, 'r') as file_obj:\n",
        "        data = json.load(file_obj)\n",
        "\n",
        "    questions = data['questions']\n",
        "\n",
        "    if data['data_subtype'] == 'test-dev2015':\n",
        "        file_prefix = 'test2015'\n",
        "    else:\n",
        "        file_prefix = data['data_subtype']\n",
        "\n",
        "    if is_labeled:\n",
        "        # For labeled data (training or validation set)\n",
        "        annotation_pattern = os.path.join(annotations_dir, f'*{file_prefix}*.json')\n",
        "        annotation_path = glob.glob(annotation_pattern)[0]\n",
        "\n",
        "        with open(annotation_path, 'r') as file_obj:\n",
        "            annotations = json.load(file_obj)['annotations']\n",
        "\n",
        "        question_annotations = {ann['question_id']: ann for ann in annotations}\n",
        "\n",
        "        find_top_answer.unknown_answers_count = 0\n",
        "\n",
        "    dataset = [None] * len(questions)\n",
        "\n",
        "    for idx, question in enumerate(questions):\n",
        "        if (idx + 1) % 10000 == 0:\n",
        "            print(f'Processing {data[\"data_subtype\"]} data: {idx + 1}/{len(questions)}')\n",
        "\n",
        "        question_id = question['question_id']\n",
        "        question_sentence = question['question']\n",
        "        image_id = question['image_id']\n",
        "        image_name = f'COCO_{file_prefix}_{image_id:012d}.jpg'\n",
        "\n",
        "        data_entry = [image_name, question_sentence]\n",
        "\n",
        "        if is_labeled:\n",
        "            annotation_answer = question_annotations[question_id]['multiple_choice_answer']\n",
        "            answer = find_top_answer(annotation_answer)\n",
        "            data_entry.append(answer)\n",
        "\n",
        "        dataset[idx] = data_entry\n",
        "\n",
        "    if is_labeled:\n",
        "        print(f'Total {find_top_answer.unknown_answers_count} out of {len(questions)} answers are unknown')\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1R6oTWcituD",
        "outputId": "3982690b-48ea-4515-fd36-cd347a4b4f87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val\n",
            "Processing val2014 data: 10000/214354\n",
            "Processing val2014 data: 20000/214354\n",
            "Processing val2014 data: 30000/214354\n",
            "Processing val2014 data: 40000/214354\n",
            "Processing val2014 data: 50000/214354\n",
            "Processing val2014 data: 60000/214354\n",
            "Processing val2014 data: 70000/214354\n",
            "Processing val2014 data: 80000/214354\n",
            "Processing val2014 data: 90000/214354\n",
            "Processing val2014 data: 100000/214354\n",
            "Processing val2014 data: 110000/214354\n",
            "Processing val2014 data: 120000/214354\n",
            "Processing val2014 data: 130000/214354\n",
            "Processing val2014 data: 140000/214354\n",
            "Processing val2014 data: 150000/214354\n",
            "Processing val2014 data: 160000/214354\n",
            "Processing val2014 data: 170000/214354\n",
            "Processing val2014 data: 180000/214354\n",
            "Processing val2014 data: 190000/214354\n",
            "Processing val2014 data: 200000/214354\n",
            "Processing val2014 data: 210000/214354\n",
            "Total 27454 out of 214354 answers are unknown\n",
            "train\n",
            "Processing train2014 data: 10000/443757\n",
            "Processing train2014 data: 20000/443757\n",
            "Processing train2014 data: 30000/443757\n",
            "Processing train2014 data: 40000/443757\n",
            "Processing train2014 data: 50000/443757\n",
            "Processing train2014 data: 60000/443757\n",
            "Processing train2014 data: 70000/443757\n",
            "Processing train2014 data: 80000/443757\n",
            "Processing train2014 data: 90000/443757\n",
            "Processing train2014 data: 100000/443757\n",
            "Processing train2014 data: 110000/443757\n",
            "Processing train2014 data: 120000/443757\n",
            "Processing train2014 data: 130000/443757\n",
            "Processing train2014 data: 140000/443757\n",
            "Processing train2014 data: 150000/443757\n",
            "Processing train2014 data: 160000/443757\n",
            "Processing train2014 data: 170000/443757\n",
            "Processing train2014 data: 180000/443757\n",
            "Processing train2014 data: 190000/443757\n",
            "Processing train2014 data: 200000/443757\n",
            "Processing train2014 data: 210000/443757\n",
            "Processing train2014 data: 220000/443757\n",
            "Processing train2014 data: 230000/443757\n",
            "Processing train2014 data: 240000/443757\n",
            "Processing train2014 data: 250000/443757\n",
            "Processing train2014 data: 260000/443757\n",
            "Processing train2014 data: 270000/443757\n",
            "Processing train2014 data: 280000/443757\n",
            "Processing train2014 data: 290000/443757\n",
            "Processing train2014 data: 300000/443757\n",
            "Processing train2014 data: 310000/443757\n",
            "Processing train2014 data: 320000/443757\n",
            "Processing train2014 data: 330000/443757\n",
            "Processing train2014 data: 340000/443757\n",
            "Processing train2014 data: 350000/443757\n",
            "Processing train2014 data: 360000/443757\n",
            "Processing train2014 data: 370000/443757\n",
            "Processing train2014 data: 380000/443757\n",
            "Processing train2014 data: 390000/443757\n",
            "Processing train2014 data: 400000/443757\n",
            "Processing train2014 data: 410000/443757\n",
            "Processing train2014 data: 420000/443757\n",
            "Processing train2014 data: 430000/443757\n",
            "Processing train2014 data: 440000/443757\n",
            "Total 55843 out of 443757 answers are unknown\n",
            "test\n",
            "Processing test2015 data: 10000/447793\n",
            "Processing test2015 data: 20000/447793\n",
            "Processing test2015 data: 30000/447793\n",
            "Processing test2015 data: 40000/447793\n",
            "Processing test2015 data: 50000/447793\n",
            "Processing test2015 data: 60000/447793\n",
            "Processing test2015 data: 70000/447793\n",
            "Processing test2015 data: 80000/447793\n",
            "Processing test2015 data: 90000/447793\n",
            "Processing test2015 data: 100000/447793\n",
            "Processing test2015 data: 110000/447793\n",
            "Processing test2015 data: 120000/447793\n",
            "Processing test2015 data: 130000/447793\n",
            "Processing test2015 data: 140000/447793\n",
            "Processing test2015 data: 150000/447793\n",
            "Processing test2015 data: 160000/447793\n",
            "Processing test2015 data: 170000/447793\n",
            "Processing test2015 data: 180000/447793\n",
            "Processing test2015 data: 190000/447793\n",
            "Processing test2015 data: 200000/447793\n",
            "Processing test2015 data: 210000/447793\n",
            "Processing test2015 data: 220000/447793\n",
            "Processing test2015 data: 230000/447793\n",
            "Processing test2015 data: 240000/447793\n",
            "Processing test2015 data: 250000/447793\n",
            "Processing test2015 data: 260000/447793\n",
            "Processing test2015 data: 270000/447793\n",
            "Processing test2015 data: 280000/447793\n",
            "Processing test2015 data: 290000/447793\n",
            "Processing test2015 data: 300000/447793\n",
            "Processing test2015 data: 310000/447793\n",
            "Processing test2015 data: 320000/447793\n",
            "Processing test2015 data: 330000/447793\n",
            "Processing test2015 data: 340000/447793\n",
            "Processing test2015 data: 350000/447793\n",
            "Processing test2015 data: 360000/447793\n",
            "Processing test2015 data: 370000/447793\n",
            "Processing test2015 data: 380000/447793\n",
            "Processing test2015 data: 390000/447793\n",
            "Processing test2015 data: 400000/447793\n",
            "Processing test2015 data: 410000/447793\n",
            "Processing test2015 data: 420000/447793\n",
            "Processing test2015 data: 430000/447793\n",
            "Processing test2015 data: 440000/447793\n",
            "test-dev\n",
            "Processing test-dev2015 data: 10000/107394\n",
            "Processing test-dev2015 data: 20000/107394\n",
            "Processing test-dev2015 data: 30000/107394\n",
            "Processing test-dev2015 data: 40000/107394\n",
            "Processing test-dev2015 data: 50000/107394\n",
            "Processing test-dev2015 data: 60000/107394\n",
            "Processing test-dev2015 data: 70000/107394\n",
            "Processing test-dev2015 data: 80000/107394\n",
            "Processing test-dev2015 data: 90000/107394\n",
            "Processing test-dev2015 data: 100000/107394\n",
            "[['COCO_train2014_000000458752.jpg', 'What is this photo taken looking through?', 'net'], ['COCO_train2014_000000458752.jpg', 'What position is this man playing?', 'pitcher'], ['COCO_train2014_000000458752.jpg', 'What color is the players shirt?', 'orange']]\n"
          ]
        }
      ],
      "source": [
        "def process_question_files(questions_path, annotations_path):\n",
        "\n",
        "    preprocessed_data = {}\n",
        "\n",
        "    for file_name in os.listdir(questions_path):\n",
        "        try:\n",
        "            data_type = file_name[20:-19]\n",
        "            print(data_type)\n",
        "\n",
        "            is_labeled = \"test\" not in data_type\n",
        "\n",
        "            question_file = os.path.join(questions_path, file_name)\n",
        "\n",
        "            preprocessed_data[data_type] = preprocess_data(question_file, annotations_path, is_labeled)\n",
        "\n",
        "        except (IOError, SyntaxError):\n",
        "            pass\n",
        "\n",
        "    print(preprocessed_data['train'][:3])\n",
        "    return preprocessed_data\n",
        "\n",
        "# Call the function to process question files\n",
        "processed_data = process_question_files(questions_path, annotations_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WhTVyCFTi2Vl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def save_processed_data(processed_data, output_path):\n",
        "\n",
        "    for data_type, data_entries in processed_data.items():\n",
        "        # Convert the list of data entries to a NumPy array\n",
        "        data_array = np.array(data_entries)\n",
        "        output_file_path = os.path.join(output_path, f'{data_type}.npy')\n",
        "\n",
        "        # Save the NumPy array to the output file\n",
        "        np.save(output_file_path, data_array)\n",
        "\n",
        "save_processed_data(processed_data, output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "o52OnNQ3i3hk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "preprocessed_data_path = \"/content/drive/MyDrive/DL_Project/preprocessed\"\n",
        "vqa_preprocessed_path = \"/content/drive/MyDrive/DL_Project/VQA_preprocessed\"\n",
        "\n",
        "def copy_preprocessed_data(src_dir, dest_dir):\n",
        "    os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "    # List of file names to copy\n",
        "    file_names = [\"test-dev.npy\", \"test.npy\", \"train.npy\", \"val.npy\"]\n",
        "\n",
        "    # Copy the preprocessed data files\n",
        "    for file_name in file_names:\n",
        "        src_path = os.path.join(src_dir, file_name)\n",
        "        dest_path = os.path.join(dest_dir, file_name)\n",
        "        shutil.copy(src_path, dest_path)\n",
        "\n",
        "    # Copy the vocabulary files\n",
        "    questions_dir = os.path.join(src_dir, \"Questions\")\n",
        "    annotations_dir = os.path.join(src_dir, \"Annotations\")\n",
        "\n",
        "    shutil.copy(os.path.join(questions_dir, \"question_vocabs.txt\"), dest_dir)\n",
        "    shutil.copy(os.path.join(annotations_dir, \"annotation_vocabs.txt\"), dest_dir)\n",
        "\n",
        "copy_preprocessed_data(preprocessed_data_path, vqa_preprocessed_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
